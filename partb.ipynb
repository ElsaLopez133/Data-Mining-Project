{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import collections\n",
    "import auxiliary_functions\n",
    "import pprint\n",
    "import json\n",
    "import random\n",
    "from itertools import combinations\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10000, 13)\n",
      "Query:  ['gargae_sm=2', 'floors=2']\n",
      "Dataframe of query:    nrooms nbedrooms nbath   sm garden_sm floors gargae_sm price year windows  \\\n",
      "0    NaN       NaN   NaN  NaN       NaN      2         2   NaN  NaN     NaN   \n",
      "\n",
      "  dist_city doors  \n",
      "0       NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data_house/database.csv\", sep = ',') \n",
    "column_names = data.columns\n",
    "n = len(data.columns)\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "\n",
    "# Generate a random query   \n",
    "m = random.randint(1,4)\n",
    "df = data.sample(n = m, axis = 'columns').sample()\n",
    "row = []\n",
    "df_fake_queries = pd.DataFrame(index = range(1), columns = column_names)\n",
    "df_fake_queries.drop(df_fake_queries.columns[df_fake_queries.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "\n",
    "for j in range(len(df.columns)):\n",
    "    row.append(''.join((str(df.columns[j]),'=',str(df.iloc[0][j]))))\n",
    "    df_fake_queries[str(df.columns[j])].iloc[0] = df.iloc[0][j]\n",
    "\n",
    "print('Query: ', row)\n",
    "print('Dataframe of query: ', df_fake_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>nrooms</th>\n",
       "      <th>nbedrooms</th>\n",
       "      <th>nbath</th>\n",
       "      <th>sm</th>\n",
       "      <th>garden_sm</th>\n",
       "      <th>floors</th>\n",
       "      <th>gargae_sm</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>windows</th>\n",
       "      <th>dist_city</th>\n",
       "      <th>doors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id nrooms nbedrooms nbath   sm garden_sm floors gargae_sm price year  \\\n",
       "0     10000    NaN       NaN   NaN  NaN       NaN      2         2   NaN  NaN   \n",
       "\n",
       "  windows dist_city doors  \n",
       "0     NaN       NaN   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = df_fake_queries.dropna(axis = 1)\n",
    "query_columns = query.columns\n",
    "query_values = query.values[0]\n",
    "random_query = df_fake_queries\n",
    "random_query.insert(0,'query_id',10000)\n",
    "random_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query does not exist in the database\n"
     ]
    }
   ],
   "source": [
    "# We have to check if the query already exists in our query database\n",
    "queries_temp =  pd.read_csv(\"./data_house/queries_labels.csv\", sep = ',', index_col = 0)\n",
    "queries =  pd.read_csv(\"./data_house/queries_labels.csv\", sep = ',')\n",
    "resData = queries.merge(df_fake_queries, how = 'inner' , on=['nrooms','nbedrooms','nbath','sm','garden_sm','floors','gargae_sm','price','year','windows','dist_city','doors'])\n",
    "if resData.empty:\n",
    "    print('The query does not exist in the database')\n",
    "    case = 1\n",
    "else:\n",
    "    print('The query already exists')\n",
    "    case = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the query exists in the dataframe, we can simply base the importance of that query in the ranking that we have provided in part A. In case it does not exist, we cna look for queries with common conditions and find the similarity between these found queries and the top 5 ranked queries for each user, ranked by themselves and not \"by us\" in part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_queries =  pd.read_csv(\"./data_house/user_queries.csv\", sep = ',')\n",
    "user_queries_fill = pd.read_csv(\"./data_house/user_queries_fill.csv\", sep = ',')\n",
    "importance = pd.DataFrame(0, index = range(len(user_queries)), columns =['user_id','importance_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if case == 0:\n",
    "    resData_idx = resData['query_id_x'].tolist()\n",
    "    for i in range(len(user_queries)):\n",
    "        rank = []\n",
    "        for j in range(len(resData_idx)):\n",
    "            rank.append(user_queries_fill.loc[i,str(j)])\n",
    "            \n",
    "        importance['user_id'].iloc[i] = user_queries['user_id'].iloc[i]\n",
    "        importance['importance_value'].iloc[i] = np.mean(rank)\n",
    "\n",
    "    print(importance[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 2: up to 2 common value\n",
      "['floors', 'floors', 'gargae_sm', 'gargae_sm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elsal\\OneDrive\\Documentos\\Master\\EIT\\Trento\\DataMining\\project\\auxiliary_functions.py:227: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  idx = list(queries[queries[str(query_columns[0])] == query.iloc[0,0]][queries[str(query_columns[1])] == query.iloc[0,1]].index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------user 1------------\n",
      " \n",
      "Top 5 ranked queries:  ['18', '481', '629', '647', '885']\n",
      "Rank of top 5 ranked queries:  [89.0, 88.0, 87.0, 86.0, 85.0]\n",
      "Most similar queries to each of the top ranked:  [1399, 1599, 1726, 544, 681]\n",
      "Value of most similar queries to each of the top ranked:  [0.63, 0.44, 0.55, 0.44, 0.5]\n",
      "Similarity of top ranked by user and original query:  [0.04, 0.0, 0.0, 0.0, 0.0]\n",
      "similarity of most similar wrt original query:  [0.06, 0.05, 0.07, 0.1, 0.25]\n",
      "[0.04, 0.02, 0.04, 0.04, 0.12]\n",
      "---------------user 2------------\n",
      " \n",
      "Top 5 ranked queries:  ['26', '169', '206', '324', '388']\n",
      "Rank of top 5 ranked queries:  [59.0, 58.0, 57.0, 56.0, 55.0]\n",
      "Most similar queries to each of the top ranked:  [1913, 68, 1420, 212, 336]\n",
      "Value of most similar queries to each of the top ranked:  [0.45, 0.67, 0.46, 0.64, 0.67]\n",
      "Similarity of top ranked by user and original query:  [0.0, 0.15, 0.0, 0.0, 0.0]\n",
      "similarity of most similar wrt original query:  [0.36, 0.14, 0.33, 0.12, 0.14]\n",
      "[0.16, 0.09, 0.15, 0.08, 0.09]\n",
      "---------------user 3------------\n",
      " \n",
      "Top 5 ranked queries:  ['116', '226', '294', '334', '373']\n",
      "Rank of top 5 ranked queries:  [59.0, 58.0, 57.0, 56.0, 55.0]\n",
      "Most similar queries to each of the top ranked:  [1014, 111, 1621, 1456, 1014]\n",
      "Value of most similar queries to each of the top ranked:  [0.58, 0.14, 0.52, 0.37, 0.42]\n",
      "Similarity of top ranked by user and original query:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "similarity of most similar wrt original query:  [0.07, 0.38, 0.1, 0.17, 0.07]\n",
      "[0.04, 0.05, 0.05, 0.06, 0.03]\n"
     ]
    }
   ],
   "source": [
    "if case == 1:\n",
    "    dict_query = {}\n",
    "    # We create a dictionary with the possible combinations:\n",
    "    comb, l = auxiliary_functions.combination(query_columns)\n",
    "    for i in range(l):\n",
    "        dict_query.update({str(comb[i]) : []} )\n",
    "    length = len(query_columns)\n",
    "    dict_query = auxiliary_functions.matching_queries(length, query_columns, query, dict_query, queries)\n",
    "    \n",
    "    # For the queries in the dictionary we create the query set to calculate similaritis later\n",
    "    all_values = dict_query.values()\n",
    "    index_values_dict = list(chain.from_iterable(list(all_values)))\n",
    "    sim_queries = queries.iloc[index_values_dict]\n",
    "    \n",
    "    # We can calculate the similarity between the original query and the ones that share some conditions\n",
    "    random_query_set = list(auxiliary_functions.queries_as_sets(random_query, filename='random_query.json').values())[0]\n",
    "    print(random_query_set)\n",
    "    similarity_value_query = pd.DataFrame(0, index = range(len(sim_queries)), columns = ['query_id', 'sim_value'])\n",
    "    similarity_value_query['query_id'] = sim_queries['query_id'].tolist()\n",
    "    similarity = []\n",
    "\n",
    "    for query_id in sim_queries['query_id']:\n",
    "        gvn_jsonfile = open(\"query_set.json\")\n",
    "        json_data = json.load(gvn_jsonfile)\n",
    "        set_query = json_data[str(query_id)]\n",
    "        \n",
    "        similarity.append(auxiliary_functions.jaccard_similarity(random_query_set, set_query))\n",
    "\n",
    "    similarity_value_query['sim_value'] = similarity\n",
    "    \n",
    "    \n",
    "    for i in range(3):\n",
    "        gvn_jsonfile = open(\"query_set.json\")\n",
    "        json_data = json.load(gvn_jsonfile)\n",
    "\n",
    "        print(\"---------------user {}------------\\n \".format(i+1))\n",
    "        dict_cluster_sim = {}\n",
    "        user_queries_non_nan = []\n",
    "        user_queries_non_nan_rank = []\n",
    "        user_queries_nan = []\n",
    "        \n",
    "        # We create lists containing the indexes of no ranked queries and ranked queries\n",
    "        for t,j in user_queries.iloc[i][1:].items():           \n",
    "            if (np.isnan(j)):\n",
    "                user_queries_nan.append(t)\n",
    "            else:\n",
    "                user_queries_non_nan.append(t)\n",
    "                user_queries_non_nan_rank.append(j)\n",
    "        n_nan_queries = len(user_queries_nan)\n",
    "        \n",
    "        # We look at the 5 highest ranked queries\n",
    "        top_5_index_queries = auxiliary_functions.sort_by_indexes(user_queries_non_nan, user_queries_non_nan_rank, True)[:5]\n",
    "        top_5_rank_queries = auxiliary_functions.find_highest_values(user_queries_non_nan_rank, ordered_nums_to_return=5)\n",
    "        print('Top 5 ranked queries: ', top_5_index_queries)\n",
    "        print('Rank of top 5 ranked queries: ',top_5_rank_queries )\n",
    "                \n",
    "        #print('Visited queries: ', len(user_queries_non_nan))\n",
    "        #print('Not visited queries: ', len(user_queries_nan))\n",
    "\n",
    "        # Create a dictionary\n",
    "        for j in range(len(np.unique(queries['kmeans_label_id']))):\n",
    "            dict_cluster_sim.update({str(np.unique(queries['kmeans_label_id'])[j]) : []})\n",
    "        \n",
    "        for k in range(len(sim_queries)):\n",
    "            dict_cluster_sim[str(queries['kmeans_label_id'].iloc[k])].append(sim_queries['query_id'].iloc[k])\n",
    "\n",
    "        index_top_sim = [0,0,0,0,0]\n",
    "        value_top_sim = [0,0,0,0,0]\n",
    "        sim_item_original = []\n",
    "        for t, item in enumerate(top_5_index_queries):\n",
    "            set_query_non_nan = json_data[str(item)]\n",
    "            key = str(queries['kmeans_label_id'].iloc[int(item)]) \n",
    "            # We calculate  the similarity between the top ranked and the original query\n",
    "            sim_item_original.append(round(auxiliary_functions.jaccard_similarity(set_query_non_nan, random_query_set),2))\n",
    "            \n",
    "            sim_value = 0\n",
    "            for query_id in dict_cluster_sim[key]:\n",
    "                gvn_jsonfile = open(\"query_set.json\")\n",
    "                json_data = json.load(gvn_jsonfile)\n",
    "                set_query = json_data[str(query_id)]\n",
    "                similarity_value = auxiliary_functions.jaccard_similarity(set_query_non_nan, set_query)\n",
    "                \n",
    "                if similarity_value > sim_value:\n",
    "                    sim_value = round(similarity_value,2)\n",
    "                    value_top_sim[t] = sim_value\n",
    "                    index_top_sim[t] = query_id\n",
    "                    \n",
    "                \n",
    "                '''\n",
    "                if similarity_value > min(value_top_sim):\n",
    "                    min_index = value_top_sim.index(min(value_top_sim))\n",
    "                    index_top_sim[min_index] = int(query_id)\n",
    "                    value_top_sim[min_index] = round(similarity_value,2) \n",
    "                '''\n",
    "    \n",
    "        print('Most similar queries to each of the top ranked: ', index_top_sim)\n",
    "        print('Similarity of most similar queries to each of the top ranked: ', value_top_sim)\n",
    "        print('Similarity of top ranked by user and original query: ', sim_item_original)\n",
    "        \n",
    "        similarity_query_random = []\n",
    "        for l in range(len(index_top_sim)):\n",
    "            similarity_query_random.append(round(similarity_value_query['sim_value'][similarity_value_query['query_id'] ==index_top_sim[l]].tolist()[0],2))\n",
    "            \n",
    "        print('similarity of most similar wrt original query: ', similarity_query_random)\n",
    "        \n",
    "        weights = [round(i*j,2) for i,j in zip(similarity_query_random, value_top_sim)]\n",
    "        print(weights)\n",
    "        #importance = \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can base the importance or relevance of a query in different aspects:\n",
    "<ul>\n",
    "    <li>  The ranking of queries with similar value of Jaccard similarity</li>\n",
    "    <li> The ranking of queries that share some of the condition and their values </li>\n",
    "    <li> The number of queries that were already posed by the user and have similar conditions </li> \n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d7fc9756e6d3d4b3fe7869fe2ad8d9e8627485ab5c06687c17be4604983b9ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
